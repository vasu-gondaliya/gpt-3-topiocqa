{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.similarities import WmdSimilarity\n",
    "import re \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_articles(text):\n",
    "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "    return re.sub(regex, ' ', text)\n",
    "def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def cleaning(text):\n",
    "    text = remove_articles(text)\n",
    "    text = white_space_fix(text)\n",
    "    text = remove_punc(text)\n",
    "    text = lower(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WMD(sentence1, sentence2):\n",
    "    # Cleaning\n",
    "    # tokenized_sentence_1 = cleaning(sentence1).split()\n",
    "    # tokenized_sentence_2 = cleaning(sentence2).split()\n",
    "    # Tokenize the sentences\n",
    "#     tokenized_sentence_1 = sentence1.lower().split()\n",
    "#     tokenized_sentence_2 = sentence2.lower().split()\n",
    "#     Compute the Word Mover's Distance between the two sentences\n",
    "    # wmd_instance = WmdSimilarity([tokenized_sentence_1], model, num_best=1)\n",
    "    # distance = wmd_instance[tokenized_sentence_2][0]\n",
    "\n",
    "    distance = model.wmdistance(sentence1, sentence2)\n",
    "\n",
    "    # model = Word2Vec()\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Mover's Distance: 0.16435539943358113\n"
     ]
    }
   ],
   "source": [
    "# # Define two sentences to compare\n",
    "# sentence_1 = 'The quick brown fox jumps over the lazy dog.'\n",
    "# sentence_2 = 'The quick brown fox jumps lazy dog.'\n",
    "\n",
    "# # Print the distance\n",
    "# print(\"Word Mover's Distance:\", WMD(sentence_1, sentence_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Previous: Type 45 destroyer\\n\\nCurrent: Type 45 destroyer\\n\\nQuestion: what was one of the provisions of the petition of right (1628)\\nAnswer: No person should be forced to provide a gift, loan or tax without an Act of Parliament.\\nQuestion: what does the term \"petition of right\" refer to?\\nAnswer: It is an English constitutional document setting out specific individual protections against the state.\\nQuestion: who formulated it?\\nAnswer: Charles I\\nQuestion: what kinds of provisions does it have?\\nAnswer: No person should be forced to provide a gift, loan or tax without an Act of Parliament. No free individual should be imprisoned or detained unless a cause has been shown. Soldiers or members of the Royal Navy should not be billeted in private houses without the free consent of the owner.\\nQuestion: which period marked the age of sail for the navy mentioned above?\\nAnswer: During the reign of Henry VIII.\\nQuestion: do they have an escort fleet?\\nAnswer: Yes, there are six Type 45 destroyers and 13 Type 23 frigates in active service.\\nQuestion: what is type 45?\\nAnswer: It is also known as the D or \"Daring\" class. It is a class of six guided missile destroyers built for the United Kingdom\\'s Royal Navy.\\nQuestion: did its engine face any trouble?\\nAnswer: On occasion there have been near-complete power generation failures, temporarily disabling propulsion, power generation for weapons, navigational systems and other purposes, leaving the ships vulnerable to \"total electric failure\".\\nQuestion: how strong was their air defense?\\nAnswer:\\n\\n###\\n\\n', 'completion': ' The Royal Navy describes the destroyers\\' mission as being \"to shield the Fleet from air attack\". It is equipped with the Sea Viper air-defence system utilizing the SAMPSON active electronically scanned array multi-function radar and the S1850M long-range radar. PAAMS is able to track over 2,000 targets and simultaneously control and coordinate multiple missiles in the air at once, allowing a large number of tracks to be intercepted and destroyed at any given time. END'}\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# !pip install jsonlines\n",
    "\n",
    "import jsonlines\n",
    "\n",
    "gold_file_path = \"gpt3_dev_context_topic_topic_data-2.jsonl\"\n",
    "\n",
    "gold_array = []\n",
    "gold_completion = []\n",
    "with jsonlines.open(gold_file_path, 'r') as file:\n",
    "  gold_array = [obj for obj in file]\n",
    "  for pc in gold_array:\n",
    "    # print(pc[\"completion\"][:-4])\n",
    "    gold_completion.append(pc[\"completion\"][:-4])\n",
    "\n",
    "print(gold_array[0])\n",
    "print(len(gold_completion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "pred_file_path = \"pred_ctt_final.json\"\n",
    "pred_file = open(pred_file_path)\n",
    "pred_completion = json.load(pred_file)\n",
    "print(len(pred_completion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "    return re.sub(regex, ' ', text)\n",
    "  def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(s):\n",
    "  if not s: return []\n",
    "  # stop_words = set(stopwords.words('english'))\n",
    "  # stop_words = ['stop', 'the', 'to', 'and', 'a', 'in', 'it', 'is', 'i', 'that', 'had', 'on', 'for', 'were', 'was', 'an', 'this']\n",
    "  # s_split = normalize_answer(s).split()\n",
    "  # s_wo_stop = [w for w in s_split if not w in stop_words]\n",
    "\n",
    "  return normalize_answer(s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Topic\n",
      "0.4868479070952298\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "distances = []\n",
    "for i, _ in enumerate(gold_array):\n",
    "    distances.append(WMD(gold_completion[i], pred_completion[i]))\n",
    "    # print()\n",
    "    # gold_toks = get_tokens(gold_completion[i])\n",
    "    # pred_toks = get_tokens(pred_completion[i])\n",
    "print(\"Context Topic\")\n",
    "print(np.array(distances).mean())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Topic Topic\n",
      "0.5054471690379486\n"
     ]
    }
   ],
   "source": [
    "distances = []\n",
    "for i, _ in enumerate(gold_array):\n",
    "    distances.append(WMD(gold_completion[i], pred_completion[i]))\n",
    "    # print()\n",
    "    # gold_toks = get_tokens(gold_completion[i])\n",
    "    # pred_toks = get_tokens(pred_completion[i])\n",
    "print(\"Context Topic Topic\")\n",
    "print(np.array(distances).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Topic\n",
      "0.5617109766144751\n"
     ]
    }
   ],
   "source": [
    "distances = []\n",
    "for i, _ in enumerate(gold_array):\n",
    "    distances.append(WMD(gold_completion[i], pred_completion[i]))\n",
    "    # print()\n",
    "    # gold_toks = get_tokens(gold_completion[i])\n",
    "    # pred_toks = get_tokens(pred_completion[i])\n",
    "print(\"Topic Topic\")\n",
    "print(np.array(distances).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Data Topic Data\n",
      "0.6621865539660716\n"
     ]
    }
   ],
   "source": [
    "distances = []\n",
    "for i, _ in enumerate(gold_array):\n",
    "    distances.append(WMD(gold_completion[i], pred_completion[i]))\n",
    "    # print()\n",
    "    # gold_toks = get_tokens(gold_completion[i])\n",
    "    # pred_toks = get_tokens(pred_completion[i])\n",
    "print(\"Topic Data Topic Data\")\n",
    "print(np.array(distances).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Answer\n",
      "0.5762059544608569\n"
     ]
    }
   ],
   "source": [
    "distances = []\n",
    "for i, _ in enumerate(gold_array):\n",
    "    distances.append(WMD(gold_completion[i], pred_completion[i]))\n",
    "    # print()\n",
    "    # gold_toks = get_tokens(gold_completion[i])\n",
    "    # pred_toks = get_tokens(pred_completion[i])\n",
    "print(\"Question Answer\")\n",
    "print(np.array(distances).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTT 1k\n",
      "0.5149877667731495\n"
     ]
    }
   ],
   "source": [
    "distances = []\n",
    "for i, _ in enumerate(gold_array):\n",
    "    distances.append(WMD(gold_completion[i], pred_completion[i]))\n",
    "    # print()\n",
    "    # gold_toks = get_tokens(gold_completion[i])\n",
    "    # pred_toks = get_tokens(pred_completion[i])\n",
    "print(\"CTT 1k\")\n",
    "print(np.array(distances).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
